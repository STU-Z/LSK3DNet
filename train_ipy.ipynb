{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4300b9bb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def main_worker(local_rank, nprocs, configs):\n",
    "    torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b462f5b5",
   "metadata": {},
   "source": [
    "定义了主工作进程函数，local_rank 表示当前进程/显卡编号，nprocs 是总进程数（即GPU数），configs 是配置参数。\n",
    "torch.autograd.set_detect_anomaly(True) 用于自动检测反向传播中的异常（如梯度为nan或inf），方便调试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f0bbee",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    dataset_config = configs['dataset_params']\n",
    "    model_config = configs['model_params']\n",
    "    train_hypers = configs['train_params']\n",
    "    sparse_config = configs['sparse_params']\n",
    "    train_hypers.local_rank = local_rank\n",
    "    train_hypers.world_size = nprocs\n",
    "    configs.train_params.world_size = nprocs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18fec17",
   "metadata": {},
   "source": [
    "读取配置文件中的各类参数，方便后续使用。\n",
    "设置当前进程的 local_rank 和总进程数 world_size。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8478d5d6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    # 初始化分布式训练环境\n",
    "    if train_hypers['distributed']:\n",
    "        init_method = 'tcp://' + args.ip + ':' + args.port\n",
    "        dist.init_process_group(\n",
    "            backend='nccl', init_method=init_method, world_size=nprocs, rank=local_rank)\n",
    "        dataset_config.train_data_loader.batch_size = dataset_config.train_data_loader.batch_size // nprocs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78de5c2",
   "metadata": {},
   "source": [
    "如果启用了分布式训练（多GPU），则初始化分布式环境：\n",
    "init_method 指定通信方式和端口。\n",
    "dist.init_process_group 初始化分布式通信，nccl 是NVIDIA推荐的多GPU通信后端。\n",
    "按GPU数等比例缩小每个进程的数据加载 batch size，保证总batch size不变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a20573",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    pytorch_device = torch.device('cuda:' + str(local_rank))\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.cuda.set_device(local_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0144e02",
   "metadata": {},
   "source": [
    "设置当前进程使用的GPU编号。\n",
    "启用cudnn的自动优化（加速卷积）。\n",
    "显式指定当前进程使用哪块GPU。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7e2436",
   "metadata": {},
   "source": [
    "总结：\n",
    "这段代码的主要作用是：\n",
    "\n",
    "为每个进程分配对应的GPU和参数，\n",
    "如果是多GPU分布式训练，则初始化分布式环境，\n",
    "并设置好数据加载和设备环境，为后续模型训练做准备。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4177bc69",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "seed = train_hypers.seed + local_rank * \\\n",
    "        dataset_config.train_data_loader.num_workers * \\\n",
    "        train_hypers['max_num_epochs']\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fc505e",
   "metadata": {},
   "source": [
    "含义解释：\n",
    "\n",
    "seed 的计算\n",
    "\n",
    "train_hypers.seed：基础随机种子（可以在配置文件中设置）。\n",
    "local_rank：当前进程/显卡编号（多卡分布式时每个进程不同）。\n",
    "dataset_config.train_data_loader.num_workers：每个DataLoader的工作线程数。\n",
    "train_hypers['max_num_epochs']：最大训练轮数。\n",
    "这三者相乘后加到基础种子上，确保每个进程/每张卡的随机种子都不一样，避免多卡训练时数据增强、采样等完全一样，提升训练多样性和复现性。\n",
    "设置随机种子\n",
    "\n",
    "random.seed(seed)：设置Python标准库的随机种子。\n",
    "np.random.seed(seed)：设置Numpy的随机种子。\n",
    "torch.manual_seed(seed)：设置PyTorch的CPU随机种子。\n",
    "torch.cuda.manual_seed(seed)：设置PyTorch的GPU随机种子。\n",
    "总结：\n",
    "这段代码的作用是为当前进程设置独特的随机种子，保证多卡训练时每个进程的随机行为不同，同时保证实验可复现性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fede2c7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "   SemKITTI_label_name = get_SemKITTI_label_name(\n",
    "        dataset_config[\"label_mapping\"])\n",
    "    unique_label = np.asarray(sorted(list(SemKITTI_label_name.keys())))[1:] - 1\n",
    "    unique_label_str = [SemKITTI_label_name[x] for x in unique_label + 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2450d4c",
   "metadata": {},
   "source": [
    "具体来说，get_SemKITTI_label_name 的作用是：\n",
    "\n",
    "读取配置文件，拿到 learning_map 和 labels 两个字段。\n",
    "遍历 learning_map 的每个原始标签ID（key），\n",
    "用 learning_map[i] 得到训练用的类别ID（value），\n",
    "用 labels[i] 得到原始标签ID对应的类别名称，\n",
    "最终生成一个字典：key 是训练用类别ID，value 是类别名称。\n",
    "例如，如果 learning_map 里有 10: 1，labels 里有 10: \"car\"，那么 SemKITTI_label_name[1] = \"car\"。\n",
    "\n",
    "总结：\n",
    "SemKITTI_label_name 是“训练类别ID → 类别名称”的映射，它用到了 learning_map 和 labels 两个字段，不是只保存了 learning_map 字段。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc78183",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    " my_model = get_model_class(model_config['model_architecture'])(configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a36717",
   "metadata": {},
   "source": [
    "含义解释：\n",
    "\n",
    "model_config['model_architecture']\n",
    "\n",
    "读取配置文件中的模型结构名称，比如 \"largekernelseg\"（见你的 lk-semantickitti_sub_tta.yaml）。\n",
    "get_model_class(...)\n",
    "\n",
    "这是一个工厂函数，根据模型名称返回对应的模型类。例如，如果传入 \"largekernelseg\"，就会返回 LargeKernelSeg 这个类（定义在 largekernel_model.py 里）。\n",
    "(...) (configs)\n",
    "\n",
    "括号里的 configs 表示实例化模型类，把所有配置参数传给模型的构造函数，得到一个模型对象。\n",
    "总结：\n",
    "这行代码的作用是：\n",
    "\n",
    "根据配置文件选择模型结构，并用当前配置参数实例化一个模型对象 my_model。\n",
    "\n",
    "这样后续就可以用 my_model 进行训练和推理了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6d4ff4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if train_hypers['distributed']:\n",
    "        my_model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(my_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087dbaeb",
   "metadata": {},
   "source": [
    "含义解释：\n",
    "\n",
    "这段代码的作用是：如果启用了分布式训练（多GPU），就把模型中的所有 BatchNorm 层转换为 SyncBatchNorm 层。\n",
    "SyncBatchNorm（同步批归一化）可以在多卡训练时，把所有 GPU 上的 batch 统计信息同步，保证归一化效果一致，提升模型收敛和精度。\n",
    "convert_sync_batchnorm(my_model) 会递归地把模型里的所有 nn.BatchNorm* 层替换成 nn.SyncBatchNorm 层。\n",
    "总结：\n",
    "\n",
    "多卡分布式训练时，自动把模型里的 BatchNorm 层变成同步版，保证训练效果。\n",
    "单卡训练时不会做任何改变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e529417",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    # 加载预训练权重\n",
    "    if os.path.exists(model_config['model_load_path']):\n",
    "        print('pre-train')\n",
    "        try:\n",
    "            my_model, pre_weight = load_checkpoint_model_mask(\n",
    "                model_config['model_load_path'], my_model, pytorch_device)\n",
    "        except:\n",
    "            my_model = load_checkpoint_old(\n",
    "                model_config['model_load_path'], my_model)\n",
    "\n",
    "    my_model.to(pytorch_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6043f3b2",
   "metadata": {},
   "source": [
    "含义解释：\n",
    "\n",
    "判断是否存在预训练模型文件\n",
    "if os.path.exists(model_config['model_load_path']):\n",
    "检查配置文件中指定的 model_load_path 路径下是否有预训练模型权重文件。\n",
    "\n",
    "加载预训练权重\n",
    "\n",
    "如果存在，打印 'pre-train'。\n",
    "首先尝试用 load_checkpoint_model_mask 加载权重（这个函数可能会返回模型和掩码参数，适用于稀疏化训练）。\n",
    "如果加载失败（比如权重文件格式不对），就用 load_checkpoint_old 以兼容老格式的方式加载权重。\n",
    "模型放到指定设备\n",
    "my_model.to(pytorch_device)\n",
    "把模型移动到当前进程对应的 GPU 上。\n",
    "\n",
    "总结：\n",
    "这段代码的作用是：\n",
    "\n",
    "如果有预训练模型权重文件，就加载到当前模型，并把模型放到对应的 GPU 上。\n",
    "这样可以实现模型的断点续训、微调或直接用预训练参数初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295c0810",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    # 设置分布式训练和混合精度\n",
    "    if train_hypers['distributed']:\n",
    "        train_hypers.local_rank = train_hypers.local_rank % torch.cuda.device_count()\n",
    "        my_model = DistributedDataParallel(\n",
    "            my_model, device_ids=[train_hypers.local_rank], find_unused_parameters=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cdef16",
   "metadata": {},
   "source": [
    "含义解释：\n",
    "\n",
    "判断是否分布式训练\n",
    "只有在多卡分布式训练（distributed=True）时才会执行下面的操作。\n",
    "\n",
    "规范 local_rank\n",
    "train_hypers.local_rank = train_hypers.local_rank % torch.cuda.device_count()\n",
    "这一步确保 local_rank 不会超过实际 GPU 数量，防止索引越界。\n",
    "\n",
    "模型封装为分布式模型\n",
    "my_model = DistributedDataParallel(...)\n",
    "\n",
    "用 PyTorch 的 DistributedDataParallel（DDP）把模型包装起来，实现多卡同步训练。\n",
    "device_ids=[train_hypers.local_rank] 指定当前进程使用哪块 GPU。\n",
    "find_unused_parameters=False 表示模型的所有参数都参与训练（如果有些参数没用到，可以设为 True）。\n",
    "总结：\n",
    "这段代码的作用是：\n",
    "\n",
    "在分布式训练时，把模型用 DDP 封装，实现多卡同步训练。\n",
    "这样每个进程只负责一张卡，参数梯度会自动同步，提升训练效率和一致性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af56403d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "  # 构建数据加载器\n",
    "    train_dataset_loader, val_dataset_loader, train_sampler = data_builder.build(\n",
    "        dataset_config, train_hypers)\n",
    "\n",
    "    configs.train_params.total_steps = train_hypers['max_num_epochs'] * len(\n",
    "        train_dataset_loader)\n",
    "    print(len(train_dataset_loader))\n",
    "    sparse_config['stop_sparse_epoch'] = sparse_config['stop_sparse_epoch'] * \\\n",
    "        len(train_dataset_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cdc67d",
   "metadata": {},
   "source": [
    "构建数据加载器和一些配置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155925b4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    " # 构建优化器和学习率调度器\n",
    "    optimizer, scheduler = optim_builder.build(configs, my_model)\n",
    "    # 构建损失函数\n",
    "    criterion = loss_builder.criterion(configs, pytorch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbebfbaa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "  # 设置混合精度训练\n",
    "    scaler = amp.GradScaler(enabled=train_hypers['amp_enabled'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ed676b",
   "metadata": {},
   "source": [
    "1. 作用\n",
    "这行代码的作用是初始化一个梯度缩放器（GradScaler）对象，用于 PyTorch 的混合精度训练（Automatic Mixed Precision, AMP）。\n",
    "\n",
    "2. 背景知识\n",
    "混合精度训练（AMP）：指同时使用 float16（半精度）和 float32（单精度）进行训练，可以大幅提升训练速度、减少显存占用。\n",
    "梯度缩放（GradScaler）：由于 float16 精度较低，反向传播时梯度容易下溢变成0，导致训练不稳定。GradScaler 会自动把 loss 放大若干倍，反向传播后再缩小梯度，避免下溢。\n",
    "3. 参数说明\n",
    "amp.GradScaler：PyTorch 官方的梯度缩放工具，位于 torch.cuda.amp 模块下。\n",
    "enabled=train_hypers['amp_enabled']：只有当配置文件中 amp_enabled 为 True 时才启用混合精度，否则 GradScaler 不起作用（等价于不用混合精度）。\n",
    "\n",
    "总结\n",
    "这一句的作用是：\n",
    "\n",
    "初始化混合精度训练的梯度缩放器对象，为后续训练过程中的自动混合精度和梯度缩放做准备，从而提升训练效率并节省显存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039cff0b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    " # 设置模型稀疏化\n",
    "    if sparse_config['use_sparse']:\n",
    "        decay = CosineDecay(sparse_config['prune_rate'], int(\n",
    "            configs.train_params.total_steps))\n",
    "        mask = Masking(optimizer, scaler,\n",
    "                       spatial_partition=model_config['spatial_group_partition'],\n",
    "                       prune_mode=sparse_config['prune'], prune_rate_decay=decay,\n",
    "                       growth_mode=sparse_config['growth'], redistribution_mode=sparse_config['redistribution'],\n",
    "                       fp16=train_hypers['amp_enabled'], update_frequency=sparse_config['update_frequency'],\n",
    "                       sparsity=sparse_config['sparsity'], sparse_init=sparse_config['sparse_init'],\n",
    "                       device=train_hypers.local_rank, distributed=train_hypers['distributed'], stop_iter=sparse_config['stop_sparse_epoch'])\n",
    "        try:\n",
    "            mask.add_module(my_model, pre_weight)\n",
    "        except:\n",
    "            mask.add_module(my_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4eb81c",
   "metadata": {},
   "source": [
    "整体作用\n",
    "这段代码的作用是：\n",
    "如果启用稀疏训练，则初始化稀疏化控制器 Masking，并将模型参数注册进去，后续训练过程中会自动进行参数剪枝和生长，实现动态稀疏训练。\n",
    "\n",
    "这样可以让模型在训练过程中自动变得稀疏（大部分参数为零），从而减少计算量和显存占用，提高推理效率。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
